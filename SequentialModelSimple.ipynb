{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LUteaching.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcosma/DeepLearningTutorials/blob/master/SequentialModelSimple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg1TmIbqdRV6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Lecture: Create your first Sequential Model in Keras by Dr Georgina Cosma**\n",
        "\n",
        "This tutorial is based on  \n",
        "\n",
        "https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WAVoe6SpMZU",
        "colab_type": "text"
      },
      "source": [
        "Learning outcomes:\n",
        "* Load Data.\n",
        "* Define a Sequential model.\n",
        "* Compile Model.\n",
        "* Evaluate Model.\n",
        "\n",
        "\n",
        "**Not using Colab?** If you are not using Colab you will need to Setup a Python Environment for \n",
        "Machine Learning and Deep Learning with Anaconda. You must have Python 2 or 3 \n",
        "installed and configured. You must install SciPy (including NumPy) and the relevant\n",
        "libraries including Keras. \n",
        "\n",
        "**Using Colab:** Some difficulties may be\n",
        "experienced with mounting, but the code and explanation here will help you overcome these. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52Rq10MgkEOM",
        "colab_type": "text"
      },
      "source": [
        "**Step 1: Import libraries** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJYbRn_jdSBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I34xdjC4kTVk",
        "colab_type": "text"
      },
      "source": [
        "**Step 2: Mount to Google Drive in order to access your data file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqvH4wcpkbRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "248b9e64-d854-4ec1-8a5f-916ae2fd933b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#!ls \"/content/drive/My Drive/Colab Notebooks\"\n",
        "\n",
        "#if you need to remount\n",
        "#drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "#If you want to unmount and reset then: \n",
        "#Step 1: From the menu select Runtime--->Reset all Runtimes... \n",
        "#Step 2: Runtime--->Run all or you can run each Cell at a time. There will be a message \n",
        "# \"Go to a URL in a browser\" and you must click on that and copy and paste the authorisation code \n",
        "# from the page into the authorisation code text box\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuRvKRH9kn5s",
        "colab_type": "text"
      },
      "source": [
        "**Step 3: Whenever we work with machine learning algorithms that use a stochastic process (e.g. random numbers), it is a good idea to set the random number seed.\n",
        "Setting the random seed, results in getting the same outputs whenever you run the code. \n",
        "Setting the random seed is useful if you need to demonstrate a result, compare algorithms using the same source of randomness or to debug a part of your code. **\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDEjzHwNavrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stFb9r6kkNLX",
        "colab_type": "code",
        "outputId": "d60ed171-a61d-4757-cefc-f6e946e9736b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# load the dataset\n",
        "dataset = numpy.loadtxt(\"/content/drive/My Drive/Colab Notebooks/pima.txt\", delimiter=\",\")\n",
        "print(dataset)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  6.    148.     72.    ...   0.627  50.      1.   ]\n",
            " [  1.     85.     66.    ...   0.351  31.      0.   ]\n",
            " [  8.    183.     64.    ...   0.672  32.      1.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...   0.245  30.      0.   ]\n",
            " [  1.    126.     60.    ...   0.349  47.      1.   ]\n",
            " [  1.     93.     70.    ...   0.315  23.      0.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3utKVAwj4Iu",
        "colab_type": "text"
      },
      "source": [
        "**Step 4: Split input (X) and output (Y) variables into separate matrices.**\n",
        "\n",
        "* **Dataset contains 8 features (or variables)** \n",
        "* **Number of input variables: 8. Columns 0 to 8**   \n",
        "* **Number of output variables: 1. Column 8**\n",
        "\n",
        "* **Let X be the m x n feature by case matrix. In this example a case is a person's data.**\n",
        "* **Let Y be a 1 x n vector holding all the labels. One row in Y corresponds to a row in X.**\n",
        "\n",
        "**Dataset has 9 columns and the range 0:8 will select columns from 0 to 7, stopping before index 8.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBmDJLsWk64e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aff4d7f0-159d-4773-def8-15663d2dc6de"
      },
      "source": [
        "#Split into X and Y variables\n",
        "X = dataset[0:760,0:8] #select all the inputs 0 to 7\n",
        "Y = dataset[0:760,8] #label is the last column, column 8\n",
        "\n",
        "#remove some records for validating the model\n",
        "X1 = dataset[760:768,0:8]\n",
        "Y1 = dataset[760:768,8]\n",
        "\n",
        "#check the size of matrix X\n",
        "#dataset.shape\n",
        "X.shape\n",
        "X1.shape\n",
        "#print record of dataset X and X1\n",
        "#print(X[759])\n",
        "#print(X1[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSvPiJjpoah9",
        "colab_type": "text"
      },
      "source": [
        "**Step 5: Create the Sequential model**\n",
        "A Sequential model is a linear stack of layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25NjmEwUk9Tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify the model \n",
        "#First layer in the Sequential model\n",
        "model = Sequential()\n",
        "#Dense(n) is a fully-connected layer with x hidden units.  \n",
        "#In the first layer, you must specify the expected input data shape: input_dim=8 dimensional vectors.\n",
        "\n",
        "# Line model.add(Dense(12, input_dim=8, activation='relu', name='input_layer')), does the following:\n",
        "#defines the input layer as having 8 inputs.\n",
        "#defines a hidden layer with 12 neurons, connected to the input layer that use relu activation function.\n",
        "#initializes all weights using a sample of uniform random numbers.\n",
        "\n",
        "model.add(Dense(12, input_dim=8, activation='relu', name='input_layer'))\n",
        "# now the model will take as input arrays of shape (*, 8)\n",
        "# and output arrays of shape (*, 12)\n",
        "\n",
        "# after the first layer, you don't need to specify\n",
        "# the size of the input anymore.\n",
        "\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Rrmq-G61sO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Step 6: Compile defines the loss function, the optimizer and the metrics.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KmlQzemk_uM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gfkomZVorVl",
        "colab_type": "text"
      },
      "source": [
        "**Step 7: Fit the model. fit() is for training the model with the given inputs (and corresponding training labels). Adjust the number of epochs to  avoid overtraining your network.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-2INuUKlBjU",
        "colab_type": "code",
        "outputId": "7dcf3431-14c7-4ca0-c350-cd3a318d90ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5134
        }
      },
      "source": [
        "model.fit(X, Y, epochs=150, batch_size=10)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "760/760 [==============================] - 0s 496us/step - loss: 4.1796 - acc: 0.6250\n",
            "Epoch 2/150\n",
            "760/760 [==============================] - 0s 137us/step - loss: 1.0067 - acc: 0.5776\n",
            "Epoch 3/150\n",
            "760/760 [==============================] - 0s 122us/step - loss: 0.7440 - acc: 0.6605\n",
            "Epoch 4/150\n",
            "760/760 [==============================] - 0s 124us/step - loss: 0.7030 - acc: 0.6618\n",
            "Epoch 5/150\n",
            "760/760 [==============================] - 0s 127us/step - loss: 0.6730 - acc: 0.6750\n",
            "Epoch 6/150\n",
            "760/760 [==============================] - 0s 120us/step - loss: 0.6466 - acc: 0.6789\n",
            "Epoch 7/150\n",
            "760/760 [==============================] - 0s 121us/step - loss: 0.6573 - acc: 0.6763\n",
            "Epoch 8/150\n",
            "760/760 [==============================] - 0s 150us/step - loss: 0.6336 - acc: 0.6882\n",
            "Epoch 9/150\n",
            "760/760 [==============================] - 0s 162us/step - loss: 0.6263 - acc: 0.7000\n",
            "Epoch 10/150\n",
            "760/760 [==============================] - 0s 118us/step - loss: 0.6192 - acc: 0.6947\n",
            "Epoch 11/150\n",
            "760/760 [==============================] - 0s 116us/step - loss: 0.6195 - acc: 0.6816\n",
            "Epoch 12/150\n",
            "760/760 [==============================] - 0s 125us/step - loss: 0.6201 - acc: 0.6882\n",
            "Epoch 13/150\n",
            "760/760 [==============================] - 0s 117us/step - loss: 0.6365 - acc: 0.6724\n",
            "Epoch 14/150\n",
            "760/760 [==============================] - 0s 117us/step - loss: 0.6202 - acc: 0.6816\n",
            "Epoch 15/150\n",
            "760/760 [==============================] - 0s 111us/step - loss: 0.6273 - acc: 0.6829\n",
            "Epoch 16/150\n",
            "760/760 [==============================] - 0s 120us/step - loss: 0.6068 - acc: 0.6947\n",
            "Epoch 17/150\n",
            "760/760 [==============================] - 0s 111us/step - loss: 0.6011 - acc: 0.6908\n",
            "Epoch 18/150\n",
            "760/760 [==============================] - 0s 124us/step - loss: 0.5960 - acc: 0.7013\n",
            "Epoch 19/150\n",
            "760/760 [==============================] - 0s 111us/step - loss: 0.5746 - acc: 0.7118\n",
            "Epoch 20/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.5964 - acc: 0.7026\n",
            "Epoch 21/150\n",
            "760/760 [==============================] - 0s 111us/step - loss: 0.5698 - acc: 0.7197\n",
            "Epoch 22/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.5867 - acc: 0.7066\n",
            "Epoch 23/150\n",
            "760/760 [==============================] - 0s 116us/step - loss: 0.5730 - acc: 0.6895\n",
            "Epoch 24/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5705 - acc: 0.7184\n",
            "Epoch 25/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5720 - acc: 0.7184\n",
            "Epoch 26/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.5666 - acc: 0.7171\n",
            "Epoch 27/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.5566 - acc: 0.7224\n",
            "Epoch 28/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5670 - acc: 0.7145\n",
            "Epoch 29/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.5580 - acc: 0.7171\n",
            "Epoch 30/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5588 - acc: 0.7250\n",
            "Epoch 31/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.5791 - acc: 0.7066\n",
            "Epoch 32/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.5508 - acc: 0.7289\n",
            "Epoch 33/150\n",
            "760/760 [==============================] - 0s 110us/step - loss: 0.5505 - acc: 0.7303\n",
            "Epoch 34/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.5491 - acc: 0.7276\n",
            "Epoch 35/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.5393 - acc: 0.7382\n",
            "Epoch 36/150\n",
            "760/760 [==============================] - 0s 116us/step - loss: 0.5468 - acc: 0.7263\n",
            "Epoch 37/150\n",
            "760/760 [==============================] - 0s 102us/step - loss: 0.5391 - acc: 0.7250\n",
            "Epoch 38/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5479 - acc: 0.7382\n",
            "Epoch 39/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.5486 - acc: 0.7408\n",
            "Epoch 40/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.5443 - acc: 0.7250\n",
            "Epoch 41/150\n",
            "760/760 [==============================] - 0s 102us/step - loss: 0.5379 - acc: 0.7289\n",
            "Epoch 42/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.5294 - acc: 0.7461\n",
            "Epoch 43/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.5322 - acc: 0.7184\n",
            "Epoch 44/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.5525 - acc: 0.7118\n",
            "Epoch 45/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5311 - acc: 0.7263\n",
            "Epoch 46/150\n",
            "760/760 [==============================] - 0s 113us/step - loss: 0.5547 - acc: 0.7184\n",
            "Epoch 47/150\n",
            "760/760 [==============================] - 0s 111us/step - loss: 0.5282 - acc: 0.7368\n",
            "Epoch 48/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5323 - acc: 0.7355\n",
            "Epoch 49/150\n",
            "760/760 [==============================] - 0s 103us/step - loss: 0.5682 - acc: 0.7105\n",
            "Epoch 50/150\n",
            "760/760 [==============================] - 0s 117us/step - loss: 0.5283 - acc: 0.7579\n",
            "Epoch 51/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.5279 - acc: 0.7447\n",
            "Epoch 52/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.5315 - acc: 0.7263\n",
            "Epoch 53/150\n",
            "760/760 [==============================] - 0s 103us/step - loss: 0.5206 - acc: 0.7513\n",
            "Epoch 54/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.5275 - acc: 0.7355\n",
            "Epoch 55/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.5318 - acc: 0.7553\n",
            "Epoch 56/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.5213 - acc: 0.7461\n",
            "Epoch 57/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.5456 - acc: 0.7408\n",
            "Epoch 58/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.5289 - acc: 0.7408\n",
            "Epoch 59/150\n",
            "760/760 [==============================] - 0s 129us/step - loss: 0.5292 - acc: 0.7342\n",
            "Epoch 60/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.5342 - acc: 0.7316\n",
            "Epoch 61/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.5322 - acc: 0.7289\n",
            "Epoch 62/150\n",
            "760/760 [==============================] - 0s 110us/step - loss: 0.5136 - acc: 0.7434\n",
            "Epoch 63/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5143 - acc: 0.7355\n",
            "Epoch 64/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.5167 - acc: 0.7592\n",
            "Epoch 65/150\n",
            "760/760 [==============================] - 0s 112us/step - loss: 0.5078 - acc: 0.7539\n",
            "Epoch 66/150\n",
            "760/760 [==============================] - 0s 116us/step - loss: 0.5295 - acc: 0.7368\n",
            "Epoch 67/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.5488 - acc: 0.7145\n",
            "Epoch 68/150\n",
            "760/760 [==============================] - 0s 101us/step - loss: 0.5180 - acc: 0.7500\n",
            "Epoch 69/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.5128 - acc: 0.7461\n",
            "Epoch 70/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.5069 - acc: 0.7566\n",
            "Epoch 71/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5078 - acc: 0.7539\n",
            "Epoch 72/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.5141 - acc: 0.7539\n",
            "Epoch 73/150\n",
            "760/760 [==============================] - 0s 103us/step - loss: 0.5056 - acc: 0.7500\n",
            "Epoch 74/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.5261 - acc: 0.7250\n",
            "Epoch 75/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.5156 - acc: 0.7566\n",
            "Epoch 76/150\n",
            "760/760 [==============================] - 0s 111us/step - loss: 0.5329 - acc: 0.7408\n",
            "Epoch 77/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.5097 - acc: 0.7566\n",
            "Epoch 78/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5030 - acc: 0.7605\n",
            "Epoch 79/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.5048 - acc: 0.7697\n",
            "Epoch 80/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.4977 - acc: 0.7684\n",
            "Epoch 81/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.5023 - acc: 0.7618\n",
            "Epoch 82/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.5064 - acc: 0.7539\n",
            "Epoch 83/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.5129 - acc: 0.7395\n",
            "Epoch 84/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5176 - acc: 0.7500\n",
            "Epoch 85/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5158 - acc: 0.7632\n",
            "Epoch 86/150\n",
            "760/760 [==============================] - 0s 112us/step - loss: 0.5145 - acc: 0.7447\n",
            "Epoch 87/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.5033 - acc: 0.7671\n",
            "Epoch 88/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.5075 - acc: 0.7395\n",
            "Epoch 89/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5031 - acc: 0.7447\n",
            "Epoch 90/150\n",
            "760/760 [==============================] - 0s 110us/step - loss: 0.5057 - acc: 0.7526\n",
            "Epoch 91/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.5103 - acc: 0.7539\n",
            "Epoch 92/150\n",
            "760/760 [==============================] - 0s 111us/step - loss: 0.5013 - acc: 0.7553\n",
            "Epoch 93/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.4972 - acc: 0.7750\n",
            "Epoch 94/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.5003 - acc: 0.7632\n",
            "Epoch 95/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.4942 - acc: 0.7526\n",
            "Epoch 96/150\n",
            "760/760 [==============================] - 0s 112us/step - loss: 0.4897 - acc: 0.7671\n",
            "Epoch 97/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.4963 - acc: 0.7697\n",
            "Epoch 98/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.4948 - acc: 0.7539\n",
            "Epoch 99/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.4913 - acc: 0.7750\n",
            "Epoch 100/150\n",
            "760/760 [==============================] - 0s 103us/step - loss: 0.4934 - acc: 0.7592\n",
            "Epoch 101/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.5046 - acc: 0.7579\n",
            "Epoch 102/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.4902 - acc: 0.7579\n",
            "Epoch 103/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.5033 - acc: 0.7434\n",
            "Epoch 104/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.4939 - acc: 0.7553\n",
            "Epoch 105/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.4878 - acc: 0.7526\n",
            "Epoch 106/150\n",
            "760/760 [==============================] - 0s 101us/step - loss: 0.5226 - acc: 0.7408\n",
            "Epoch 107/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.4877 - acc: 0.7632\n",
            "Epoch 108/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4896 - acc: 0.7737\n",
            "Epoch 109/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.4932 - acc: 0.7592\n",
            "Epoch 110/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.4925 - acc: 0.7645\n",
            "Epoch 111/150\n",
            "760/760 [==============================] - 0s 103us/step - loss: 0.4890 - acc: 0.7553\n",
            "Epoch 112/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.4919 - acc: 0.7763\n",
            "Epoch 113/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.4991 - acc: 0.7632\n",
            "Epoch 114/150\n",
            "760/760 [==============================] - 0s 108us/step - loss: 0.5003 - acc: 0.7618\n",
            "Epoch 115/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4895 - acc: 0.7658\n",
            "Epoch 116/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4970 - acc: 0.7750\n",
            "Epoch 117/150\n",
            "760/760 [==============================] - 0s 110us/step - loss: 0.4814 - acc: 0.7737\n",
            "Epoch 118/150\n",
            "760/760 [==============================] - 0s 103us/step - loss: 0.5041 - acc: 0.7513\n",
            "Epoch 119/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4849 - acc: 0.7711\n",
            "Epoch 120/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4891 - acc: 0.7618\n",
            "Epoch 121/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4865 - acc: 0.7724\n",
            "Epoch 122/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.4916 - acc: 0.7697\n",
            "Epoch 123/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.4901 - acc: 0.7513\n",
            "Epoch 124/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.4925 - acc: 0.7618\n",
            "Epoch 125/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.4980 - acc: 0.7579\n",
            "Epoch 126/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.4879 - acc: 0.7711\n",
            "Epoch 127/150\n",
            "760/760 [==============================] - 0s 111us/step - loss: 0.4923 - acc: 0.7618\n",
            "Epoch 128/150\n",
            "760/760 [==============================] - 0s 103us/step - loss: 0.4857 - acc: 0.7658\n",
            "Epoch 129/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.4863 - acc: 0.7539\n",
            "Epoch 130/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.4836 - acc: 0.7697\n",
            "Epoch 131/150\n",
            "760/760 [==============================] - 0s 103us/step - loss: 0.4892 - acc: 0.7566\n",
            "Epoch 132/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.4747 - acc: 0.7750\n",
            "Epoch 133/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4899 - acc: 0.7724\n",
            "Epoch 134/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4790 - acc: 0.7566\n",
            "Epoch 135/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4857 - acc: 0.7592\n",
            "Epoch 136/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.4874 - acc: 0.7632\n",
            "Epoch 137/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4735 - acc: 0.7697\n",
            "Epoch 138/150\n",
            "760/760 [==============================] - 0s 109us/step - loss: 0.4760 - acc: 0.7816\n",
            "Epoch 139/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4842 - acc: 0.7605\n",
            "Epoch 140/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4807 - acc: 0.7697\n",
            "Epoch 141/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4840 - acc: 0.7579\n",
            "Epoch 142/150\n",
            "760/760 [==============================] - 0s 102us/step - loss: 0.4794 - acc: 0.7737\n",
            "Epoch 143/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4812 - acc: 0.7724\n",
            "Epoch 144/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4789 - acc: 0.7697\n",
            "Epoch 145/150\n",
            "760/760 [==============================] - 0s 106us/step - loss: 0.4796 - acc: 0.7658\n",
            "Epoch 146/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.4728 - acc: 0.7671\n",
            "Epoch 147/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.4759 - acc: 0.7711\n",
            "Epoch 148/150\n",
            "760/760 [==============================] - 0s 105us/step - loss: 0.4711 - acc: 0.7750\n",
            "Epoch 149/150\n",
            "760/760 [==============================] - 0s 104us/step - loss: 0.4851 - acc: 0.7566\n",
            "Epoch 150/150\n",
            "760/760 [==============================] - 0s 107us/step - loss: 0.4732 - acc: 0.7671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3022c61c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbQcdVchozAV",
        "colab_type": "text"
      },
      "source": [
        "**Step 8: Evaluate the model. evaluate() is for evaluating the already trained model using the validation (or test) data and the corresponding labels. Returns the loss value and metrics values for the model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrKN4vNjlDFU",
        "colab_type": "code",
        "outputId": "5aeedc03-bc1a-45ae-9933-0a238236cbfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "scores = model.evaluate(X, Y)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "760/760 [==============================] - 0s 100us/step\n",
            "\n",
            "acc: 78.03%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4Fq-g2qpsb2",
        "colab_type": "text"
      },
      "source": [
        "**Step 9: Predict on previously unseen data. predict() is for the actual prediction. It generates output predictions for the input samples.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdoCBfuiqRkV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0b8cf66c-0256-4241-a618-1c5b773d8a53"
      },
      "source": [
        "# calculate predictions\n",
        "#X1 data was not used to train or test the model\n",
        "predictions = model.predict(X1)\n",
        "# round predictions\n",
        "rounded = [round(x1[0]) for x1 in predictions]\n",
        "print(rounded)\n",
        "\n",
        "#Simple confusion matrix using sklearn.metrics \n",
        "results = confusion_matrix(Y1, rounded)\n",
        "print(results)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
            "[[6 0]\n",
            " [0 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex7PINigW1Og",
        "colab_type": "text"
      },
      "source": [
        "**Here is the code for creating your own confusion matrix.**\n",
        "The code was taken from https://machinelearningmastery.com/implement-machine-learning-algorithm-performance-metrics-scratch-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WNKvD8bWz-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of Calculating and Displaying a Pretty Confusion Matrix\n",
        "\n",
        "#def defines a function for calculating the confusion matrix\n",
        "# calculate a confusion matrix\n",
        "def confusion_matrix(actual, predicted):\n",
        "\tunique = set(actual)\n",
        "\tmatrix = [list() for x in range(len(unique))]\n",
        "\tfor i in range(len(unique)):\n",
        "\t\tmatrix[i] = [0 for x in range(len(unique))]\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tx = lookup[actual[i]]\n",
        "\t\ty = lookup[predicted[i]]\n",
        "\t\tmatrix[y][x] += 1\n",
        "\treturn unique, matrix\n",
        " \n",
        "# pretty print a confusion matrix\n",
        "def print_confusion_matrix(unique, matrix):\n",
        "\tprint('(A)' + ' '.join(str(x) for x in unique))\n",
        "\tprint('(P)---')\n",
        "\tfor i, x in enumerate(unique):\n",
        "\t\tprint(\"%s| %s\" % (x, ' '.join(str(x) for x in matrix[i])))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF97weplXI6w",
        "colab_type": "text"
      },
      "source": [
        "**Use the confusion_matrix function to print confusion matrices for the  Predict step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn8BrvxQXG2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4398909d-addc-4515-f66e-170815cf1c5e"
      },
      "source": [
        "# Test confusion matrix for the Evaluate results\n",
        "unique, matrix = confusion_matrix(Y1, rounded)\n",
        "print_confusion_matrix(unique, matrix)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(A)0.0 1.0\n",
            "(P)---\n",
            "0.0| 6 0\n",
            "1.0| 0 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qj6zFGLlENM",
        "colab_type": "code",
        "outputId": "4c44ac92-3f3c-468c-ee86-3ac550f96739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#print the Sequential model structure\n",
        "print(model.summary())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_layer (Dense)          (None, 12)                108       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 8)                 104       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 221\n",
            "Trainable params: 221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jOK1cJbxBlU",
        "colab_type": "text"
      },
      "source": [
        "**Step 10: How to install the GraphViz Library  http://www.graphviz.org/**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPlJouVfpcIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install -q pydot\n",
        "from keras.utils.vis_utils import plot_model\n",
        "#Prints the model to a file in the directory\n",
        "plot_model(model, to_file='/content/drive/My Drive/Colab Notebooks/model_plot3.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FACsq3Oh_xDD",
        "colab_type": "text"
      },
      "source": [
        "**Further Reading:**\n",
        "About Colab: https://www.youtube.com/watch?v=inN8seMm7UI\n",
        "\n",
        "Examples to run:\n",
        "\n",
        "https://keras.io/getting-started/sequential-model-guide/#examples\n",
        "\n",
        "In the examples, you will also find example models for real datasets:\n",
        "\n",
        "* CIFAR10 small images classification: Convolutional Neural Network (CNN) with realtime data augmentation\n",
        "\n",
        "* IMDB movie review sentiment classification: LSTM over sequences of words\n",
        "Reuters newswires topic classification: Multilayer Perceptron (MLP)\n",
        "\n",
        "* MNIST handwritten digits classification: MLP & CNN\n",
        "Character-level text generation with LSTM\n"
      ]
    }
  ]
}